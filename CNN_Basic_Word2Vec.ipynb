{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Split CNN Basic",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBlnYfsjMjHe"
      },
      "source": [
        "**Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjVyeDiK6z05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgwTUAe3Mp_2"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usR3q8Aa61H4",
        "outputId": "9633924c-7d7b-458d-9a59-16abb33af129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "d = pd.read_csv('/content/drive/My Drive/Data/training_split.csv')\n",
        "x = d['tweet'].values\n",
        "\n",
        "d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>kategori</th>\n",
              "      <th>ei</th>\n",
              "      <th>ns</th>\n",
              "      <th>ft</th>\n",
              "      <th>jp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sumpah ngilu bgt gue denger lututnya chanðŸ˜©ðŸ˜©\\n\\...</td>\n",
              "      <td>ENFJ</td>\n",
              "      <td>E</td>\n",
              "      <td>N</td>\n",
              "      <td>F</td>\n",
              "      <td>J</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>syoun islam. kalo makan ga ada kursi, jongkok ...</td>\n",
              "      <td>ENFJ</td>\n",
              "      <td>E</td>\n",
              "      <td>N</td>\n",
              "      <td>F</td>\n",
              "      <td>J</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@chochoseungyoun di pintu kamar kak wkwkw</td>\n",
              "      <td>ENFJ</td>\n",
              "      <td>E</td>\n",
              "      <td>N</td>\n",
              "      <td>F</td>\n",
              "      <td>J</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@SEUNGYEONIZED aku belum mutualanðŸ¥º</td>\n",
              "      <td>ENFJ</td>\n",
              "      <td>E</td>\n",
              "      <td>N</td>\n",
              "      <td>F</td>\n",
              "      <td>J</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>duh gamau liat video call usok tp kepo tp gama...</td>\n",
              "      <td>ENFJ</td>\n",
              "      <td>E</td>\n",
              "      <td>N</td>\n",
              "      <td>F</td>\n",
              "      <td>J</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639995</th>\n",
              "      <td>@munfess jangan terlalu berlarut</td>\n",
              "      <td>ISTP</td>\n",
              "      <td>I</td>\n",
              "      <td>S</td>\n",
              "      <td>T</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639996</th>\n",
              "      <td>@gyuIaIi malah tadinya gw gamau pasang waterma...</td>\n",
              "      <td>ISTP</td>\n",
              "      <td>I</td>\n",
              "      <td>S</td>\n",
              "      <td>T</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639997</th>\n",
              "      <td>@gyuIaIi hah gemes kenapa</td>\n",
              "      <td>ISTP</td>\n",
              "      <td>I</td>\n",
              "      <td>S</td>\n",
              "      <td>T</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639998</th>\n",
              "      <td>@munfess udah ada</td>\n",
              "      <td>ISTP</td>\n",
              "      <td>I</td>\n",
              "      <td>S</td>\n",
              "      <td>T</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639999</th>\n",
              "      <td>emang harus bersih bersih gallery</td>\n",
              "      <td>ISTP</td>\n",
              "      <td>I</td>\n",
              "      <td>S</td>\n",
              "      <td>T</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>640000 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    tweet kategori ei ns ft jp\n",
              "0       sumpah ngilu bgt gue denger lututnya chanðŸ˜©ðŸ˜©\\n\\...     ENFJ  E  N  F  J\n",
              "1       syoun islam. kalo makan ga ada kursi, jongkok ...     ENFJ  E  N  F  J\n",
              "2               @chochoseungyoun di pintu kamar kak wkwkw     ENFJ  E  N  F  J\n",
              "3                      @SEUNGYEONIZED aku belum mutualanðŸ¥º     ENFJ  E  N  F  J\n",
              "4       duh gamau liat video call usok tp kepo tp gama...     ENFJ  E  N  F  J\n",
              "...                                                   ...      ... .. .. .. ..\n",
              "639995                   @munfess jangan terlalu berlarut     ISTP  I  S  T  P\n",
              "639996  @gyuIaIi malah tadinya gw gamau pasang waterma...     ISTP  I  S  T  P\n",
              "639997                          @gyuIaIi hah gemes kenapa     ISTP  I  S  T  P\n",
              "639998                                  @munfess udah ada     ISTP  I  S  T  P\n",
              "639999                  emang harus bersih bersih gallery     ISTP  I  S  T  P\n",
              "\n",
              "[640000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EENWQHrHMvZF"
      },
      "source": [
        "**Pre Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWaGKhps62Rp",
        "outputId": "35692f80-6a21-4478-fd01-b1436794c8b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import json\n",
        "\n",
        "# load data hasil pre processing\n",
        "with open('/content/drive/My Drive/Data/prepro.json', 'r') as js:\n",
        "  preprocessing = json.load(js)\n",
        "print(len(preprocessing))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCsrpQS1Swhf",
        "outputId": "b4c425cc-630f-4eee-e593-b5ad8d4c861f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "words = [item for sublist in preprocessing for item in sublist]\n",
        "print(len(words))\n",
        "print(words[0])\n",
        "print(words[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "640000\n",
            "['sumpah', 'ngilu', 'banget', 'gue', 'dengar', 'lutut', 'chan']\n",
            "sumpah\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohM9VcG063LH"
      },
      "source": [
        "max_len = 280"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29lSiJorM3Nt"
      },
      "source": [
        "**Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6oWEefN65ZC",
        "outputId": "f3a32dee-11da-4560-8c93-faf1b5d14962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "tk = Tokenizer()\n",
        "tk.fit_on_texts(words)\n",
        "x = tk.texts_to_sequences(words)\n",
        "x = sequence.pad_sequences(x, maxlen=max_len, padding='post')\n",
        "\n",
        "# pickle.dump(tk, open('/content/drive/My Drive/Data/tk.pk','wb'))\n",
        "\n",
        "# file_ = open('/content/drive/My Drive/Data/tk.pk','rb')\n",
        "# tk = pickle.load(file_)\n",
        "# file_.close()\n",
        "\n",
        "max_word = tk.word_index\n",
        "print(len(max_word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "167551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxxSSVc9M78Z"
      },
      "source": [
        "**Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBA7L37B67Yh",
        "outputId": "a9873080-3fad-4dbc-e4ac-51ff7106203e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "# path = get_tmpfile('/content/drive/My Drive/Data/word2vec_split.model')\n",
        "# model = Word2Vec(words, size=300)\n",
        "# model.wv.save_word2vec_format('/content/drive/My Drive/Data/word2vec_split.model', binary=False)\n",
        "\n",
        "vec = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Data/word2vec_split.model', binary=False)\n",
        "\n",
        "# vec.wv.vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMYnVQwK6-RK"
      },
      "source": [
        "def word_embedding(vocabulary_size,vec_dim,vec,tk):\n",
        "  matrix_embedding = np.zeros((vocabulary_size,vec_dim))\n",
        "  embedding_vector = []\n",
        "  for key,val in tk.word_index.items():\n",
        "    # jika tidak ada pada kamus diabaikan\n",
        "    if key not in vec.vocab:\n",
        "      continue\n",
        "    embedding_vector = vec[key]\n",
        "    if embedding_vector is not None:\n",
        "      matrix_embedding[val]=embedding_vector\n",
        "    else:\n",
        "      matrix_embedding[val]=unknown_vector\n",
        "  return matrix_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwuFk-vz6_Xx"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#membuat matrix embedding berisi vector kata\n",
        "num_words = len(tk.word_counts.keys())+1\n",
        "embedding_size = len(vec.wv['karena'])\n",
        "matrix_embedding = word_embedding(num_words,embedding_size,vec,tk)\n",
        "print(embedding_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MJ_DS2S7AoZ",
        "outputId": "8c3acb88-59b6-41e5-f2d8-d2c1331ae66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matrix_embedding.shape\n",
        "print(num_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "167552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs7IK0kGNB3m"
      },
      "source": [
        "**One Hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRw38b7zK7p-"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from keras import utils as np_utils\n",
        "\n",
        "kategori = 'ei'\n",
        "y = d[kategori]\n",
        "num_classes = 2\n",
        "\n",
        "# Label Encoding categorical data for the classification category\n",
        "labelencoder_Y = LabelEncoder()\n",
        "y = labelencoder_Y.fit_transform(y)\n",
        "\n",
        "# Perform one hot encoding \n",
        "y = np_utils.to_categorical(y, num_classes= num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v8-0Kp36twh"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04CkE42Agxmd"
      },
      "source": [
        "from keras.layers import Input, Dense, Flatten, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "max_features = num_words\n",
        "maxlen = max_len\n",
        "embedding_dims = embedding_size\n",
        "\n",
        "def create_model(i, x_train, x_test, y_train, y_test, kernel, dropout, learning_rate, kategori, ke):\n",
        "  print('Build model...')\n",
        "  ke = str(ke)\n",
        "  i = str(i)\n",
        "\n",
        "  model_input = Input(shape=(280,))\n",
        "  model_embedding = Embedding(num_words, embedding_size, input_length=max_len)(model_input)\n",
        "  model_conv = Conv1D(256, kernel_size=kernel, activation='relu')(model_embedding)\n",
        "  model_pool = GlobalMaxPooling1D()(model_conv)\n",
        "  model_dense = Dense(64, activation='relu')(model_pool)\n",
        "  model_dropout = Dropout(dropout)(model_dense)\n",
        "  model_output = Dense(2, activation='softmax')(model_dropout)\n",
        "\n",
        "  model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "  adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "  # es = EarlyStopping(monitor='val_loss', mode='min', patience=3)\n",
        "  mc = ModelCheckpoint('/content/drive/My Drive/Hasil/'+kategori+'_'+ke+'_'+i+'.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n",
        "  history = model.fit(x_train, y_train, batch_size=1000, validation_data=(x_test, y_test), epochs=20, callbacks=[mc])\n",
        "  \n",
        "  _, train_acc = model.evaluate(x_train, y_train)\n",
        "  _, test_acc = model.evaluate(x_test, y_test)\n",
        "  return model, train_acc, test_acc\n",
        "  # print('Train: %.4f, Test: %.4f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtDJKAwSNJvi"
      },
      "source": [
        "**K-Fold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRFdTnLH7F9b"
      },
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "# import pickle\n",
        " \n",
        "# kf = KFold(n_splits=10, random_state=7, shuffle=True)\n",
        "# kf.get_n_splits(x)\n",
        " \n",
        "# KFold(n_splits=10, random_state=7, shuffle=True)\n",
        " \n",
        "# i = 0\n",
        " \n",
        "# for train_index, test_index in kf.split(x):\n",
        "#   i = i+1\n",
        "#   x_train, x_test = x[train_index], x[test_index]\n",
        "#   y_train, y_test = y[train_index], y[test_index]\n",
        "#   j = str(i)\n",
        "#   pickle.dump(x_train, open('/content/drive/My Drive/Fold/x_train_'+kategori+'_'+j+'.pk','wb'))\n",
        "#   pickle.dump(x_test, open('/content/drive/My Drive/Fold/x_test_'+kategori+'_'+j+'.pk','wb'))\n",
        "#   pickle.dump(y_train, open('/content/drive/My Drive/Fold/y_train_'+kategori+'_'+j+'.pk','wb'))\n",
        "#   pickle.dump(y_test, open('/content/drive/My Drive/Fold/y_test_'+kategori+'_'+j+'.pk','wb'))\n",
        "#   print(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X_sYg39TMWU"
      },
      "source": [
        "import pickle\n",
        " \n",
        "for i in range(7, 10):\n",
        "  j=i+1\n",
        "  j = str(j)\n",
        "  print('Fold '+kategori+' ke - '+j)\n",
        " \n",
        "  x_train = pickle.load(open('/content/drive/My Drive/Fold/x_train_'+kategori+'_'+j+'.pk','rb'))\n",
        "  print(len(x_train))\n",
        "  x_test = pickle.load(open('/content/drive/My Drive/Fold/x_test_'+kategori+'_'+j+'.pk','rb'))\n",
        "  y_train = pickle.load(open('/content/drive/My Drive/Fold/y_train_'+kategori+'_'+j+'.pk','rb'))\n",
        "  y_test = pickle.load(open('/content/drive/My Drive/Fold/y_test_'+kategori+'_'+j+'.pk', 'rb'))\n",
        " \n",
        "  model, train_acc, test_acc = create_model(i+1, x_train, x_test, y_train, y_test, 5, 0.5, 0.00001, kategori, 8)\n",
        "  print('')\n",
        "  print('Train: %.4f, Test: %.4f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}