{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Live Crawling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EvRWHWByZaM",
        "outputId": "60f3dbec-9ef1-4d92-8f92-b6de0ce4f81d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3dRsB4z5SRo",
        "outputId": "a0cd7b53-adda-44d3-9369-70dc7e8e587c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "import re\n",
        "import pandas as pd\n",
        "import csv\n",
        "!pip install Sastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13XwA19vzldv"
      },
      "source": [
        "# Authentikasi API Twitter\n",
        "\n",
        "consumer_key = '<DROP CONSUMER KEY HERE>'\n",
        "consumer_secret = '<DROP CONSUMER SECRET HERE>'\n",
        "access_token = '<DROP ACCESS TOKEN HERE>'\n",
        "access_secret = '<DROP ACCESS SECRET HERE>'\n",
        "\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEMcrgMm0gRf"
      },
      "source": [
        "# ambil 200 tweet terbaru dari akun pengguna twitter\n",
        "# tweet yang diambil merupakan tweet berlabel bahasa indonesia dan bukan merupakan retweet murni (retweet tanpa komentar)\n",
        "\n",
        "tweets = []\n",
        "for tweet in tweepy.Cursor(api.user_timeline, id=\"<DROP ID TWITTER HERE>\", tweet_mode=\"extended\").items():\n",
        "  if len(tweets) < 200:\n",
        "    if hasattr(tweet, \"retweeted_status\"):\n",
        "      continue\n",
        "    else:\n",
        "      if tweet.lang == 'in':\n",
        "        tweets.append(tweet.full_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8M0J9bh3MJK",
        "outputId": "24b34b98-e94f-49fd-8a0d-f078102eb4cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1vLRJCU46Qd"
      },
      "source": [
        "# hapus hashtag, mention, link\n",
        "def filter(string):\n",
        "  string = re.sub(r'@[A-Za-z0-9_]+', '', string)\n",
        "  string = re.sub(r'#[A-Za-z0-9_]+', '', string)\n",
        "  string = re.sub(r'http[A-Za-z0-9_:/?.]+', '', string)\n",
        "  string = string.replace('\\n', '')\n",
        "  string = re.sub(r'\\\\[A-Za-z0-9]+', '', string)\n",
        "  return string\n",
        "\n",
        "# mengubah emoji & emoticon ke dalam kelas emosi\n",
        "def replace_emoji(string):\n",
        "  kamus = pd.read_excel('/content/drive/My Drive/Data/emoji.csv')\n",
        "  for j in range (len(kamus)):\n",
        "    string = string.replace(kamus['emoji'][j], ' :'+kamus['kategori'][j]+':')\n",
        "  return string\n",
        "\n",
        "# melakukan proses case folding\n",
        "def case_folding(string):\n",
        "  return string.lower().strip()\n",
        "\n",
        "# menghapus karakter selain huruf, angka, dan '-'\n",
        "def cleaning(string):\n",
        "  string = re.sub(r'[^a-zA-Z0-9-]', ' ', string)\n",
        "  return string\n",
        "\n",
        "# mengubah kata alay ke dalam bentuk formalnya\n",
        "def normalization(string):\n",
        "  tokens = string.split()\n",
        "  kamus = pd.read_csv('/content/drive/My Drive/Data/colloquial-indonesian-lexicon.csv')\n",
        "  text = []\n",
        "  for token in tokens:\n",
        "    for j in range(len(kamus)):\n",
        "      if token == kamus['slang'][j]:\n",
        "        token = token.replace(token, kamus['formal'][j])  \n",
        "    text.append(token)\n",
        "  string = ' '.join(text)\n",
        "  return string\n",
        "\n",
        "# menghapus angka\n",
        "def cleaning_2(string):\n",
        "  string = re.sub(r'[^a-zA-Z-]', ' ', string)\n",
        "  return string\n",
        "\n",
        "# menghapus stopword\n",
        "def stopword_removal(string):\n",
        "  factory = StopWordRemoverFactory()\n",
        "  stopword = factory.create_stop_word_remover()\n",
        "  string = stopword.remove(string)\n",
        "  return string\n",
        "\n",
        "# mengubah kata ke dalam bentuk dasarnya\n",
        "def stemming(string):\n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  string = stemmer.stem(string)\n",
        "  return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEeViyu15Ytw"
      },
      "source": [
        "# melakukan pre processing data\n",
        "def prepro(list):\n",
        "  tweets = []\n",
        "  for string in list:\n",
        "    string = filter(string)\n",
        "    string = replace_emoji(string)\n",
        "    string = case_folding(string)\n",
        "    string = cleaning(string)\n",
        "    string = normalization(string)\n",
        "    string = cleaning_2(string)\n",
        "    string = stopword_removal(string)\n",
        "    string = stemming(string)\n",
        "    token = nltk.tokenize.word_tokenize(string)\n",
        "    tweets.append(token)\n",
        "  return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbePm8ei6A0r"
      },
      "source": [
        "hasil_prepro = prepro(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ5CtaJF6TOk"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing import sequence\n",
        "import pickle\n",
        "\n",
        "tk = Tokenizer()\n",
        "file_ = open('/content/drive/My Drive/Data/tk.pk','rb')\n",
        "tk = pickle.load(file_)\n",
        "\n",
        "data_enc = tk.texts_to_sequences(hasil_prepro)\n",
        "data_pred = sequence.pad_sequences(data_enc, maxlen=280, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZO78cACxnt8",
        "outputId": "8cfa088c-781c-4e0e-fde7-e2f1a3599e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(data_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[15900   509     0 ...     0     0     0]\n",
            " [  509    25 34379 ...     0     0     0]\n",
            " [ 3576   124     2 ...     0     0     0]\n",
            " ...\n",
            " [ 5395   362     8 ...     0     0     0]\n",
            " [  509 10873     4 ...     0     0     0]\n",
            " [    0     0     0 ...     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwGACCIW-cQ_",
        "outputId": "6b3ec41f-581c-455d-866a-b8cedabc5ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# melakukan proses prediksi\n",
        "labelei = ['E', 'I']\n",
        "labelns = ['N', 'S']\n",
        "labelft = ['F', 'T']\n",
        "labeljp = ['J', 'P']\n",
        "\n",
        "modelei = load_model('/content/drive/My Drive/Live/ei_4_3.h5')\n",
        "modelns = load_model('/content/drive/My Drive/Live/ns_2_3.h5')\n",
        "modelft = load_model('/content/drive/My Drive/Live/ft_8_3.h5')\n",
        "modeljp = load_model('/content/drive/My Drive/Live/jp_8_6.h5')\n",
        "\n",
        "predictionsei = modelei.predict(data_pred)\n",
        "predictionsns = modelns.predict(data_pred)\n",
        "predictionsft = modelft.predict(data_pred)\n",
        "predictionsjp = modeljp.predict(data_pred)\n",
        "\n",
        "prediction_labels_ei=[]\n",
        "prediction_labels_ns=[]\n",
        "prediction_labels_ft=[]\n",
        "prediction_labels_jp=[]\n",
        "\n",
        "for i in range(len(predictionsei)):\n",
        "  prediction_labels_ei.append(labelei[np.argmax(predictionsei[i])])\n",
        "  prediction_labels_ns.append(labelns[np.argmax(predictionsns[i])])\n",
        "  prediction_labels_ft.append(labelft[np.argmax(predictionsft[i])])\n",
        "  prediction_labels_jp.append(labeljp[np.argmax(predictionsjp[i])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jp-g4bd-kC1",
        "outputId": "b0380aa2-90cd-4689-f336-1e99401b157a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# hasil prediksi\n",
        "\n",
        "E = prediction_labels_ei.count('E')\n",
        "I = prediction_labels_ei.count('I')\n",
        "print(\"E : \", E)\n",
        "print(\"I : \", I)\n",
        "\n",
        "N = prediction_labels_ns.count('N')\n",
        "S = prediction_labels_ns.count('S')\n",
        "print(\"N : \", N)\n",
        "print(\"S : \", S)\n",
        "\n",
        "F = prediction_labels_ft.count('F')\n",
        "T = prediction_labels_ft.count('T')\n",
        "print(\"F : \", F)\n",
        "print(\"T : \", T)\n",
        "\n",
        "J = prediction_labels_jp.count('J')\n",
        "P = prediction_labels_jp.count('P')\n",
        "print(\"J : \", J)\n",
        "print(\"P : \", P)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E :  122\n",
            "I :  78\n",
            "N :  104\n",
            "S :  96\n",
            "F :  81\n",
            "T :  119\n",
            "J :  117\n",
            "P :  83\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}