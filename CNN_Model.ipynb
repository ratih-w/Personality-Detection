{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Split CNN Basic",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBlnYfsjMjHe"
      },
      "source": [
        "**Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjVyeDiK6z05"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgwTUAe3Mp_2"
      },
      "source": [
        "**Import Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usR3q8Aa61H4"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "d = pd.read_csv('/content/drive/My Drive/Data/training_split.csv')\n",
        "x = d['tweet'].values\n",
        "\n",
        "d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EENWQHrHMvZF"
      },
      "source": [
        "**Pre Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWaGKhps62Rp"
      },
      "source": [
        "import json\n",
        "\n",
        "# load data hasil pre processing\n",
        "with open('/content/drive/My Drive/Data/data_prepro.json', 'r') as js:\n",
        "  preprocessing = json.load(js)\n",
        "print(len(preprocessing))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCsrpQS1Swhf"
      },
      "source": [
        "words = [item for sublist in preprocessing for item in sublist]\n",
        "print(len(words))\n",
        "print(words[0])\n",
        "print(words[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohM9VcG063LH"
      },
      "source": [
        "max_len = 280"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29lSiJorM3Nt"
      },
      "source": [
        "**Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6oWEefN65ZC"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "tk = Tokenizer()\n",
        "tk.fit_on_texts(words)\n",
        "x = tk.texts_to_sequences(words)\n",
        "x = sequence.pad_sequences(x, maxlen=max_len, padding='post')\n",
        "\n",
        "# pickle.dump(tk, open('/content/drive/My Drive/Data/tk.pk','wb'))\n",
        "\n",
        "# file_ = open('/content/drive/My Drive/Data/tk.pk','rb')\n",
        "# tk = pickle.load(file_)\n",
        "# file_.close()\n",
        "\n",
        "max_word = tk.word_index\n",
        "print(len(max_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxxSSVc9M78Z"
      },
      "source": [
        "**Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBA7L37B67Yh"
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "# path = get_tmpfile('/content/drive/My Drive/Data/word2vec_split.model')\n",
        "# model = Word2Vec(words, size=300)\n",
        "# model.wv.save_word2vec_format('/content/drive/My Drive/Data/word2vec_split.model', binary=False)\n",
        "\n",
        "vec = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Data/word2vec_split.model', binary=False)\n",
        "\n",
        "# vec.wv.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMYnVQwK6-RK"
      },
      "source": [
        "def word_embedding(vocabulary_size,vec_dim,vec,tk):\n",
        "  matrix_embedding = np.zeros((vocabulary_size,vec_dim))\n",
        "  embedding_vector = []\n",
        "  for key,val in tk.word_index.items():\n",
        "    # jika tidak ada pada kamus diabaikan\n",
        "    if key not in vec.vocab:\n",
        "      continue\n",
        "    embedding_vector = vec[key]\n",
        "    if embedding_vector is not None:\n",
        "      matrix_embedding[val]=embedding_vector\n",
        "    else:\n",
        "      matrix_embedding[val]=unknown_vector\n",
        "  return matrix_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwuFk-vz6_Xx"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#membuat matrix embedding berisi vector kata\n",
        "num_words = len(tk.word_counts.keys())+1\n",
        "embedding_size = len(vec.wv['karena'])\n",
        "matrix_embedding = word_embedding(num_words,embedding_size,vec,tk)\n",
        "print(embedding_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MJ_DS2S7AoZ"
      },
      "source": [
        "matrix_embedding.shape\n",
        "print(num_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs7IK0kGNB3m"
      },
      "source": [
        "**One Hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRw38b7zK7p-"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from keras import utils as np_utils\n",
        "\n",
        "kategori = 'ft'\n",
        "y = d[kategori]\n",
        "num_classes = 2\n",
        "\n",
        "# Label Encoding categorical data for the classification category\n",
        "labelencoder_Y = LabelEncoder()\n",
        "y = labelencoder_Y.fit_transform(y)\n",
        "\n",
        "# Perform one hot encoding \n",
        "y = np_utils.to_categorical(y, num_classes= num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v8-0Kp36twh"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04CkE42Agxmd"
      },
      "source": [
        "from keras.layers import Input, Dense, Flatten, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "max_features = num_words\n",
        "maxlen = max_len\n",
        "embedding_dims = embedding_size\n",
        "\n",
        "def create_model(i, x_train, x_test, y_train, y_test, kernel, dropout, learning_rate, kategori, ke):\n",
        "  print('Build model...')\n",
        "  ke = str(ke)\n",
        "  i = str(i)\n",
        "\n",
        "  model_input = Input(shape=(280,))\n",
        "  model_embedding = Embedding(max_features, embedding_size, input_length=max_len)(model_input)\n",
        "  model_conv = Conv1D(256, kernel_size=kernel, activation='relu')(model_embedding)\n",
        "  model_pool = GlobalMaxPooling1D()(model_conv)\n",
        "  model_dense = Dense(64, activation='relu')(model_pool)\n",
        "  model_dropout = Dropout(dropout)(model_dense)\n",
        "  model_output = Dense(2, activation='sigmoid')(model_dropout)\n",
        "\n",
        "  model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "  adam = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)\n",
        "  \n",
        "  _, train_acc = model.evaluate(x_train, y_train)\n",
        "  _, test_acc = model.evaluate(x_test, y_test)\n",
        "  model.save('/content/drive/My Drive/'+kategori+'_'+ke+'_'+i+'.h5')\n",
        "  return model, train_acc, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtDJKAwSNJvi"
      },
      "source": [
        "**K-Fold**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRFdTnLH7F9b"
      },
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "# import pickle\n",
        " \n",
        "# kf = KFold(n_splits=10, random_state=7, shuffle=True)\n",
        "# kf.get_n_splits(x)\n",
        " \n",
        "# KFold(n_splits=10, random_state=7, shuffle=True)\n",
        " \n",
        "# i = 0\n",
        " \n",
        "# for train_index, test_index in kf.split(x):\n",
        "#   i = i+1\n",
        "#   x_train, x_test = x[train_index], x[test_index]\n",
        "#   y_train, y_test = y[train_index], y[test_index]\n",
        "#   j = str(i)\n",
        "#   pickle.dump(x_train, open('/content/drive/My Drive/Fold/x_train_'+kategori+'_'+j+'.pk','wb'))\n",
        "#   pickle.dump(x_test, open('/content/drive/My Drive/Fold/x_test_'+kategori+'_'+j+'.pk','wb'))\n",
        "#   pickle.dump(y_train, open('/content/drive/My Drive/Fold/y_train_'+kategori+'_'+j+'.pk','wb'))\n",
        "#   pickle.dump(y_test, open('/content/drive/My Drive/Fold/y_test_'+kategori+'_'+j+'.pk','wb'))\n",
        "#   print(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X_sYg39TMWU"
      },
      "source": [
        "import pickle\n",
        " \n",
        "for i in range(0, 10):\n",
        "  j=i+1\n",
        "  j = str(j)\n",
        "  print('Fold '+kategori+' ke - '+j)\n",
        " \n",
        "  x_train = pickle.load(open('/content/drive/My Drive/Fold/x_train_'+kategori+'_'+j+'.pk','rb'))\n",
        "  print(len(x_train))\n",
        "  x_test = pickle.load(open('/content/drive/My Drive/Fold/x_test_'+kategori+'_'+j+'.pk','rb'))\n",
        "  y_train = pickle.load(open('/content/drive/My Drive/Fold/y_train_'+kategori+'_'+j+'.pk','rb'))\n",
        "  y_test = pickle.load(open('/content/drive/My Drive/Fold/y_test_'+kategori+'_'+j+'.pk', 'rb'))\n",
        " \n",
        "  model, train_acc, test_acc = create_model(i+1, x_train, x_test, y_train, y_test, 3, 0.5, 0.00001, kategori, 4)\n",
        "  \n",
        "  print('')\n",
        "  print('Train: %.4f, Test: %.4f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}